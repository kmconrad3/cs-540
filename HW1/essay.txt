CS540 HW1 AI100 Reflection
        A big concern that surfaces when discussing the growth of AI tools today is how they are biased. At one step of their lifecycle or another, since, as humans, biases are unavoidable, they seep into the products we make. Now the AI100 2021 Report makes this sound like it is inherently bad by categorizing the topic of biases in AI as a “pressing danger”, which I would like to argue the opposite. The report brings up multiple examples of how biased algorithms produce unjust results, like in the justice system, medical settings discrimination, or Amazon’s recruiting tool.
The report states “because all technology is the product of a biased system, techno-solutionism’s flaws run deep: a creation is limited by the limitations of its creator” (p. 53) which holds true, but it's the word flaws that labels natural occurring biases as bad and something we must avoid. However it is impossible to create tools that are 100% unbiased and in trying to do so, more inaccurate and deceptive tools are produced. Rather than trying to eliminate biases, engineers should accept that it is a part of reality and instead create systems that take into account that the systems themselves are created with biases. 
This requires taking a step back and accepting that AI tools will never be as intelligent and intentional as humans. And their use should be taken with a grain of salt. That salt is transparency, human supervision, avoiding black-box approaches, and diverse reviewing for starters (Ngomo, 2019).
Furthermore, biases not only need to be accepted but to certain degrees, they are necessary for machine learning. Specifically, bias in ML helps us generalize better and make our model less sensitive to some single data point in classification tools. For example, if a tool is trained on the cleanest most unbiased perfect data, when the tool is used in the real world on real data which will be biased it will have no ability to make predictions on such unseen data patterns. Removing and avoiding bias leads us to this unbiased generalizer which is unable to make a generalization based on the dataset when given an unseen data point (Gunaratne, 2020). Not only does removing biases by anti-classification, classification parity, and calibration practices create significant algorithm statistical limitations, but they may harm the very group they are designed to protect (Corbett-Davies & Goel, 2018).
        The AI100 2021 Report highlights the debate of whether we should be addressing challenges by applying general-purpose problem-solving methods, or by writing specialized algorithms for each particular problem. In order to create explainable and transparent algorithms that do deal with biases (as mentioned it is unavoidable), the second method fits best. Biases as part of creating realistic algorithms, create the tedious work of humans overseeing such algorithms. By writing specialized algorithms for each particular problem, the scope for possible unaccounted-for biases is smaller, meaning there is less for the human eye to keep track of. In part to accept that AI tools will never be as smart as humans, I believe we should avoid general-purpose problem-solving methods because they are more difficult to oversee.